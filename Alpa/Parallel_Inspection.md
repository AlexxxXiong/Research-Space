# The Methods to Inspect the Parallel Strategies Decided by ILP Solver

Content: 

- **Pre. Issues on the Representation of Parallel Method (only annotations) in Alpa**
- **Research Logs**
- **Documentations**
- **Solution**

-------



### Pre. Issues on the Representation of Parallel Method (only annotations) in Alpa

Note that the following results are only annotations generated by Alpa to pass to the ILP solver in the XLA compiler side.

After running Alpa profiling with specified model and hardware, we get the entries of `logical_mesh_shapes` and `autosharding_option_dicts` as part of the profiling result. Here we explain the meaning of these entries:

 \- There can be multiple items (corresponded to each stage) in `autosharding_option_dicts` (e.g., `[{'force_batch_dim_to_mesh_dim': 0}, {}]`) and `logical_mesh_shapes` (e.g., `[(2, 1), (2, 1)]`).

 \- For each item, if `force_batch_dim_to_mesh_dim` is set to `0`, then the first dim of the logical mesh shape is related to the batch dim and the second related to the mesh dim. For instance, `(2, 1)` denotes a vanilla data parallel, while `(2, 2)` denotes a data + model parallel.

 \- If `force_batch_dim_to_mesh_dim` is set to `None`, then the first dim of the logical mesh shape is related to the first mesh dim, which is determined by ILP solver in Alpa on applying data or model parallel. The second is not used for usual. For instance, `(2, 1)` denotes can also denote a model parallel (if better than data parallel).

------



### 1. Research Logs

寻找 stage-mesh pair 进一步进行 intra-op auto-sharding 的代码实现.

- `compile_executable.py/def shard_each_stage()`：
    - Distributed compile：从 pool 中取多个 compile workers，`run_auto_sharding_pass.remote()` 作为 `compile_fn` 被 `compile_workers.submit(compile_fn, ...)`，先 `generate_sharded_xla_computations_arguments()`，再 submit 以远程运行 `run_auto_sharding_pass()`，最后再 `generate_computations_from_modules()` 来获取 `sharded_xla_stages`；
    - Not distributed compile：直接调用 `generate_sharded_xla_computations()`，依次本地做上面三个子函数。
    - 在 `shard_each_stage()` 函数的结尾输出 `xla_stages`：不对，输出长度为 8，是 stages num * 2 (forward & backward).
- `get_input_output_sharding_specs()`：似乎是获取每个 layer input / output sharding specs 的函数，重点关注 `mesh_executable.py/class GradAccMeshDriverExecutable()` 和 `class NormalMeshDriverExecutable()` 内：
    - 在 `GradAccMeshDriverExecutable.__init__() / NormalMeshDriverExecutable.__init__()` 函数内 print：输出每个 config 各 OP 的 sharding spec.
- 论文里写到：

> TensorFlow [48], GSPMD [31, 57] and OneFlow [58] provide annotation APIs for users to manualy specifiy the intra-op parallel plan

​	之前得到的那些 `logical_mesh_shape` 之类的只是 annotation！或许可以看看 TensorFlow 怎么基于 annotation 做的 intra-op parallel.

-------



### 2. Documentations

- [Differences between alpa.parallelize, jax.pmap and jax.pjit](https://alpa.ai/tutorials/alpa_vs_pmap.html)
    - With several lines of code change, we can use `pmap` for data parallel training. However, we cannot use `pmap` for model parallel training, which is required for training large models with billions of parameters.
- [Distributed Training with Both Shard and Pipeline Parallelism](https://alpa.ai/tutorials/pipeshard_parallelism.html#interpret-the-results)
    - 我们或许可以通过添加 `alpa.mark_pipeline_boundary()` 来**人工划分 pipeline stages**（且可以根据权重划分不同的比例，在异构硬件间分配不同大小的模型参数），这需要设置 parallel method 里的 `layer_option="manual"`。但此时或许可以继续设置 `stage_option` 为 `alpa.AutoStageOption`，执行 stage-mesh pair 内部的 auto-parallel？可以的。
    - 又或许可以不这么 low-level，直接指定 layer_num  = pipeline stages num，比如指定 layer_num = 2 -> 最多只允许执行 stage_num = 2 的 pipeline.
        - <font color=red>TODO: 试一下 layer num = 4 (也是 4-GPUs) 上的 pure pipeline 是否和 layer num = 16 (4-GPUs) 的 pure pipeline 拥有相同的 performance。</font>
            - 结论：在性能波动允许的范围内，performance 相同。
    - 

> **Some basic concepts** - Cluster mesh and submeshes
>
> > - Cluster mesh is a computer cluster that contains GPUs. A `N×M` cluster mesh means the cluster has `N` physical machines and each machine has `M` GPUs.
> > - Submeshes can be obtained by slicing from the cluster mesh. For example, given a `N×M`cluster mesh, a submesh `(1, M)` means using all GPUs in one physical machine.
> > - For more details on how Alpa uses submeshes to solve *inter-operator parallelism*, you can read the **Section 5: Inter-Operator Parallelism** in the [Alpa paper](https://arxiv.org/pdf/2201.12023.pdf).
>
> - - Device mesh and logical mesh
>
>         A device mesh is a 2-dimensional logical view of a set of physical devices.For a set of physical devices, there can be multiple logical views. For example, given 2 nodes and 8 GPUs per node (i.e., 16 devices in total), we can view them as a 2×8, 1×16, 4×4, 8×2, or 16×1 device mesh.The mapping between physical devices and the logical device mesh view is optimized by the inter-op passHence, you can see `Result mesh_shapes` and the corresponding `Resultlogical_mesh_shapes` in the optimization output.
>
> With the basic concepts in mind, you now can better understand the `ModuleProfileResult`: - `ModuleProfileResult`: `result[(i, j, s, c), m]` means this stage contains forward layers `i, i+1,..., j` and corresponding backward layers, and runs under the `s`-th submesh and the `c`-th auto sharding config for the submesh. The `m = 0` means the result is for the forward pass, and `m = 1`for backward pass.

- [Performance Tuning Guide: Inspect the parallelization strategy](https://alpa.ai/tutorials/perf_tuning_guide.html#inspect-the-parallelization-strategy)

    - 介绍如何 inspect alpa 的 parallel strategies，alpa 给出 annotations 后由 xla compiler 中的 ILP solver 来决定具体的并行方式。

    > If you want to inspect the parallelization strategies, Alpa provides several debug options to dump the intermediate representations. You can see example usages at https://github.com/alpa-projects/alpa/blob/main/tests/runtime/test_debug_info.py. The key interfaces include functions `dump_debug_info`, `get_last_dp_result` and environment variable `ALPA_DEBUG_PRINT_AS_STRATEGY`. Note that Alpa does not provide nice visualization tools currently, so understanding these intermediate representations requires some knowledge of HLO and Alpa algorithms.
    >
    > See also [intra-op solver guidance](https://alpa.ai/architecture/intra_op_solver.html#learn-intra-op-solver).

- Code example: [Link](https://github.com/alpa-projects/alpa/blob/main/tests/runtime/test_debug_info.py#L34)

```python
def test_2_debug_pipeline_parallel(self):
    init(cluster="ray")
    state, batch, train_step = get_mlp_train_state_and_step(batch_size=128,
                                                            hidden_size=128,
                                                            num_layers=6)

    # Print auto-sharding intermidiate results
    global_config.pipeline_distributed_compile = False
    os.environ["ALPA_DEBUG_PRINT_AS_STRATEGY"] = "1"

    layer_num = min(get_global_cluster().num_devices, 2)
    p_train_step = parallelize(
        train_step,
        method=PipeshardParallel(
            num_micro_batches=2,
            layer_option=AutoLayerOption(layer_num=layer_num)))
    actual_output = p_train_step(state, batch)
    executable = p_train_step.get_last_executable()
    executable.sync()

    # Dump final HLO and other debug info
    executable.dump_debug_info("alpa_debug_info")

    # Print auto-stage dynamic programming results if use auto stage partition
    print(get_last_dp_result())
```

​	注意，我们可以在设置环境变量后通过 `executable.dump_debug_info(FOLDER_NAME)` 来输出模型的 sharded result.

​	输出比较重要的 `.txt` 文件是 `train_step_func_input_placement_specs.txt` 和 `train_step_func_output_placement_specs.txt`，分别代表了每个 op 输入和输出的 sharded tensor shape。例如 ResNet 的 debug log 部分如下：

```python
# train_step_func_input_placement_specs.txt

(TrainState(step=PlacementSpec(aval=ShapedArray(int32[], weak_type=True), mesh_ids=(0,), sharding_specs=(ShardingSpec((), (Replicated(replicas=2),)),)), apply_fn=<bound method Module.apply of WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 10
    width_factor = 1
    dtype = None
    act = relu
)>, params=FrozenDict({
    BottleneckResNetBlock_0: {
        BatchNorm_0: {
            bias: PlacementSpec(aval=ShapedArray(float32[10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(),), (Replicated(replicas=2),)),)),
            scale: PlacementSpec(aval=ShapedArray(float32[10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(),), (Replicated(replicas=2),)),)),
        },
        
        # Other BatchNorm...
        
        Conv_0: {
            kernel: PlacementSpec(aval=ShapedArray(float32[1,1,10,10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(), NoSharding(), NoSharding(), NoSharding()), (Replicated(replicas=2),)),)),
        },
        
        # Other Conv...
        
        conv_proj: {
            kernel: PlacementSpec(aval=ShapedArray(float32[1,1,10,40]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(), NoSharding(), NoSharding(), NoSharding()), (Replicated(replicas=2),)),)),
        },
        norm_proj: {
            bias: PlacementSpec(aval=ShapedArray(float32[40]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(),), (Replicated(replicas=2),)),)),
            scale: PlacementSpec(aval=ShapedArray(float32[40]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(),), (Replicated(replicas=2),)),)),
        },
    },
    
    # Other BottleneckResNetBlock...
  
  	tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7f6d305df160>, update=<function chain.<locals>.update_fn at 0x7f6d305df5e0>), opt_state=(TraceState(trace=FrozenDict({
    BottleneckResNetBlock_0: {
        BatchNorm_0: {
            bias: PlacementSpec(aval=ShapedArray(float32[10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((Chunked(2),), (ShardedAxis(axis=0),)),)),
            scale: PlacementSpec(aval=ShapedArray(float32[10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(),), (Replicated(replicas=2),)),)),
        },
        
      	# Other BatchNorm...
      
        Conv_0: {
            kernel: PlacementSpec(aval=ShapedArray(float32[1,1,10,10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(), NoSharding(), Chunked(2), NoSharding()), (ShardedAxis(axis=0),)),)),
        },
        
      	# Other Conv...
      
        conv_proj: {
            kernel: PlacementSpec(aval=ShapedArray(float32[1,1,10,40]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(), NoSharding(), Chunked(2), NoSharding()), (ShardedAxis(axis=0),)),)),
        },
        norm_proj: {
            bias: PlacementSpec(aval=ShapedArray(float32[40]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(),), (Replicated(replicas=2),)),)),
            scale: PlacementSpec(aval=ShapedArray(float32[40]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(),), (Replicated(replicas=2),)),)),
        },
    }
     
    # Other BottleneckResNetBlock...
      
    })), ScaleByScheduleState(count=PlacementSpec(aval=ShapedArray(int32[]), mesh_ids=(0,), sharding_specs=(ShardingSpec((), (Replicated(replicas=2),)),)))), batch_stats=FrozenDict({
    BottleneckResNetBlock_0: {
        BatchNorm_0: {
            mean: PlacementSpec(aval=ShapedArray(float32[10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((Chunked(2),), (ShardedAxis(axis=0),)),)),
            var: PlacementSpec(aval=ShapedArray(float32[10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((Chunked(2),), (ShardedAxis(axis=0),)),)),
        },
        
      	# Other BatchNorm...
      
        norm_proj: {
            mean: PlacementSpec(aval=ShapedArray(float32[40]), mesh_ids=(0,), sharding_specs=(ShardingSpec((Chunked(2),), (ShardedAxis(axis=0),)),)),
            var: PlacementSpec(aval=ShapedArray(float32[40]), mesh_ids=(0,), sharding_specs=(ShardingSpec((Chunked(2),), (ShardedAxis(axis=0),)),)),
        },
    },
      
    # Other BottleneckResNetBlock...
      
    bn_init: {
        mean: PlacementSpec(aval=ShapedArray(float32[10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((Chunked(2),), (ShardedAxis(axis=0),)),)),
        var: PlacementSpec(aval=ShapedArray(float32[10]), mesh_ids=(0,), sharding_specs=(ShardingSpec((Chunked(2),), (ShardedAxis(axis=0),)),)),
    },
}), dynamic_scale=None), {'images': PlacementSpec(aval=ShapedArray(int32[16,224,224,3]), mesh_ids=(0,), sharding_specs=(ShardingSpec((Chunked(2), NoSharding(), NoSharding(), NoSharding()), (ShardedAxis(axis=0),)),)), 'labels': PlacementSpec(aval=ShapedArray(int32[16]), mesh_ids=(0,), sharding_specs=(ShardingSpec((NoSharding(),), (Replicated(replicas=2),)),))})
      
# In train_step_func_input_placement_specs.txt, the last part should be:

# }), dynamic_scale=None), {'accuracy': PlacementSpec(aval=ShapedArray(float32[]), mesh_ids=(0,), sharding_specs=(ShardingSpec((), (Replicated(replicas=2),)),)), 'loss': PlacementSpec(aval=ShapedArray(float32[]), mesh_ids=(0,), sharding_specs=(ShardingSpec((), (Replicated(replicas=2),)),)), 'lr': PlacementSpec(aval=ShapedArray(float32[], weak_type=True), mesh_ids=(0,), sharding_specs=(ShardingSpec((), (Replicated(replicas=2),)),))})
```

[Flax Batch normalization](https://flax.readthedocs.io/en/latest/guides/batch_norm.html)

>In addition to the `params` collection, `BatchNorm` also adds a `batch_stats` collection that contains the running average of the batch statistics.
>
>Note: You can learn more in the `flax.linen` [variables](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#module-flax.core.variables) API documentation.
>
>The `batch_stats` collection must be extracted from the `variables` for later use.
>
>Flax `BatchNorm` adds a total of 4 variables: `mean` and `var` that live in the `batch_stats` collection, and `scale` and `bias` that live in the `params` collection.
>
>```
>FrozenDict({
>  'batch_stats': {
>    'BatchNorm_0': {
>        'mean': (4,),
>        'var': (4,),
>    },
>  },
>  'params': {
>    'BatchNorm_0': {
>        'bias': (4,),
>        'scale': (4,),
>    },
>    'Dense_0': {
>        'bias': (4,),
>        'kernel': (3, 4),
>    },
>    'Dense_1': {
>        'bias': (1,),
>        'kernel': (4, 1),
>    },
>  },
>})
>```

可以观察到，在 `.txt` 的输出中，分别包括：

```python
# Params related
params=FrozenDict()...
# Optax optimizer related 
# Ref: https://github.com/deepmind/optax#optax
tx=GradientTransformation(...), opt_state=(TraceState(trace=FrozenDict()),
ScaleByScheduleState(batch_stats=FrozenDict())
```

因此，我们**只需关注 `params=FrozenDict()` 内的参数划分**。

对于 `train_step_func_input_placement_specs.txt` 和 `train_step_func_input_placement_specs.txt` 的区分，阅读 `mesh_executable.py/class GradAccMeshDriverExecutable(MeshDriverExecutable)/def get_input_placement_specs()` 和 `def get_output_placement_specs()` 的实现：

```py
def dump_debug_info(self, folder: str):
    """
    Dump intermediate representations and other informations for debugging.
    """
    os.makedirs(folder, exist_ok=True)
    name = self.accumulate_grad_hlo.name
    name = name[:name.index("shard_parallel") - 1]
    prefix = os.path.join(folder, name)
    with open(f"{prefix}.hlo", "w") as f:
        f.write(self.get_hlo_text())
    with open(f"{prefix}.grad_sync_channel_ids.txt", "w") as f:
        f.write(str(self.grad_sync_channel_ids) + "\n")
    with open(f"{prefix}.mem_usage.txt", "w") as f:
        f.write(f"total_allocation_size: "
                f"{self.get_total_allocation_size()/(1024**3):.3f} GB\n")
    with open(f"{prefix}_input_placement_specs.txt", "w") as f:
        f.write(str(self.get_input_placement_specs()))
    with open(f"{prefix}_output_placement_specs.txt", "w") as f:
        f.write(str(self.get_output_placement_specs()))

def get_input_placement_specs(self):
    """
    Return the preferred placement specs for input arguments.
    The return value is a pytree of PlacementSpec
    with the same structure as the input pytree.
    """
    return wrap_to_placement_spec_tree(self.physical_mesh, self.avals,
                                       self.global_arg_sharding_specs,
                                       self.in_tree)

def get_output_placement_specs(self):
    """
    Return the preferred placement specs for outputs.
    The return value is a pytree of PlacementSpec
    with the same structure as the output pytree.
    """
    return wrap_to_placement_spec_tree(self.physical_mesh, self.out_avals,
                                       self.output_sharding_specs,
                                       self.out_tree)

# An outer function
def wrap_to_placement_spec_tree(physical_mesh, avals, sharding_specs, pytree):
    """Wrap avals and sharding specs to a pytree of placement specs."""
    placement_specs = [
        PlacementSpec(aval, (physical_mesh.mesh_id,), (sharding_spec,))
        for aval, sharding_spec in zip(avals, sharding_specs)
    ]
    return tree_unflatten(pytree, placement_specs)
```

我们注意到大部分情况下两个 `.txt` 文件下各个 OP 的 parallel method 均相等，再结合论文中的描述：

> **Parallel algorithms of an operator**. With the definitions above, consider parallelizing a batched matmul Cb;i; j = ∑k Ab;i;kBb;k; j on a 2D mesh – Table 3 lists several intra-op parallel algorithms for a batched matmul. Algorithm#1 maps loop i to the 0-th mesh axis and loop j to the 1-th mesh axis, resulting in the output tensor C with a sharding spec RS0S1. As the LHS operand Ab;i;k and RHS operand Bb;k; j both have only one parallelized index, their sharding specs are written as RS0R and RRS1, respectively. 

可以判断，只有当存在两个 inputs 时，才需要考虑某个 OP 的 input spec 和 output spec 不同的情况。因此，我们**考虑使用 OP 的 output spec 作为其 parallel method 的依据**。

- [Code Structure of the Intra-op Solver](https://alpa.ai/architecture/intra_op_solver.html#learn-intra-op-solver)

    > ### Inspect the sharding strategy
    >
    > You can print the HLO before and after the `run_auto_sharding_pass`.

    - 我们尝试在 `run_auto_sharding_pass` (in `auto_sharding.py`) 函数内的首尾分别输出`hlo`：
    
        ```python
        # In def run_auto_sharding_pass()
        
        # pylint: disable=unused-argument
        # Set compile options
        if memory_budget_per_device is None:
            memory_budget_per_device = -1
        assert hlo.is_unoptimized()
        
        # NOTE: To perform parallel inspection in alpa.
        print("")
        print("[I] The hlo module before optimization:")
        print(hlo)
        print("")
          
        #...
          
        # NOTE: To perform parallel inspection in alpa.
        print("")
        print("[I] The hlo module after optimization:")
        print(hlo)
        print("")
        
        if return_mode == "single":
            return hlo, stage_plan
        elif return_mode == "stages":
            return hlo_stage_names, hlo_stages, stage_plan
        elif return_mode == "stages_and_hook":
            return hlo_stage_names, hlo_stages, hooked_proto, stage_plan
        else:
            raise ValueError("Invalid return mode: " + return_mode)
        ```
    
        发现输出如下：
    
        ```
        (CompileWorker pid=1459255) [I] The hlo module before optimization:
        (CompileWorker pid=1459255) <alpa.wrapped_hlo.WrappedHlo object at 0x7f07080ed1f0>
        ```
    
        即输出的 hlo module 为一个 object，无法直接看到 parallel inspection。
    
    > ## How to Debug
    >
    > - Set global environment variable `ALPA_DEBUG_PRINT_AS_STRATEGY=1`. This will print the choosen sharding strategy for each instruction and edge costs in a prettier way.
    > - Check batch dim analysis https://github.com/alpa-projects/tensorflow-alpa/blob/721260d122f096040762b2d226b37e8ab23f74b8/tensorflow/compiler/xla/service/spmd/auto_sharding_util.cc#L857



-----



### 3. Solution

在 `./benchmark_parallel_utils.py/def benchmark_training_executable()` 中添加 debug 并输出中间结果的字段：

```python
# In def benchmark_training_executable():
# ...

# NOTE: Perform parallel inspection of alpa
# Print auto-sharding intermidiate results
global_config.pipeline_distributed_compile = False
os.environ["ALPA_DEBUG_PRINT_AS_STRATEGY"] = "1"

# ...

print("[I] Wait for the executable to sync...")

executable.sync()

print("[I] Executable sync is completed.")

# NOTE: Perform parallel inspection of alpa
# Dump final HLO and other debug info
executable.dump_debug_info(dump_debug_file_path)
print("[I] The result of the last dynamic programming is as follows:")
# Print auto-stage dynamic programming results if use auto stage partition
print(get_last_dp_result())

# ...
```

将相应的 debug info 保存到 `dump_debug_file_path` 内，相应 files 如下所示：

```
(base) cyxue@gpuc-01:~/Projects/playground/alpa_docker/profile_result/debug/1_nodes_2_devices_per_node/1_1080ti/wide_resnet_CIFAR10/bs_32_nmb_2_pln_16_param_num_500M$ ls
train_step_func_input_placement_specs.txt   train_step_func_stage_0.mem_usage.txt  train_step_func_stage_2.mem_usage.txt  train_step_func_stage_4.mem_usage.txt
train_step_func_output_placement_specs.txt  train_step_func_stage_1.hlo            train_step_func_stage_3.hlo            train_step_func_stage_5.hlo
train_step_func_resharding_tasks.txt        train_step_func_stage_1.mem_usage.txt  train_step_func_stage_3.mem_usage.txt  train_step_func_stage_5.mem_usage.txt
train_step_func_stage_0.hlo                 train_step_func_stage_2.hlo            train_step_func_stage_4.hlo
```

根据前述分析，我们**只需关注 `train_step_func_output_placement_specs.txt` 内 `params` 字段下每个 OP 并行策略的相关输出，并根据输出判断 `force_batch_dim_to_mesh_dim` 未设置时 ILP solver 求解的并行方式为 DP 还是 MP 即可，无需进行详细的分析。**
